{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a168bc1-c694-402f-920a-f3b35ac8fe0f",
   "metadata": {},
   "source": [
    "### Week 1: Data Science & Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d59f9-0072-47a9-b299-053664bd1ce5",
   "metadata": {},
   "source": [
    "#### Setup Environment and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52224db-5951-43f2-8d40-7301bb868fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34eb41-d827-4006-bfa5-63da12aeb1b4",
   "metadata": {},
   "source": [
    "#### Load the excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794710c5-c821-4c9d-99de-de7574132c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = pd.read_excel(\"Airlines.xlsx\")\n",
    "airports_df = pd.read_excel(\"airports.xlsx\")\n",
    "runways_df = pd.read_excel(\"runways.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1d839-cde8-4f6c-81f2-91ce98ab1585",
   "metadata": {},
   "source": [
    "#### Aggregate Runway Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf816a5-f5de-4c55-b676-343d1b270480",
   "metadata": {},
   "outputs": [],
   "source": [
    "runway_agg_df = runways_df.groupby('airport_ident').agg(\n",
    "    number_of_runways=('id', 'count'),\n",
    "    avg_runway_length_ft=('length_ft', 'mean'),\n",
    "    avg_runway_width_ft=('width_ft', 'mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50de12-b06f-4272-89b9-b0cd46098969",
   "metadata": {},
   "source": [
    "#### Merge Flights with Source and Destination Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd732a4a-84d9-4b5b-b772-c97196c62e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    flights_df,\n",
    "    airports_df,\n",
    "    left_on='AirportFrom',\n",
    "    right_on='iata_code',\n",
    "    how='left',\n",
    "    suffixes=('', '_from')\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    airports_df,\n",
    "    left_on='AirportTo',\n",
    "    right_on='iata_code',\n",
    "    how='left',\n",
    "    suffixes=('', '_to')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891a7de-13a5-495e-bd78-60911e0262e7",
   "metadata": {},
   "source": [
    "#### Merge the Aggregated Runway Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a729f4-f556-48a3-b1d1-f8f12949dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    runway_agg_df,\n",
    "    left_on='ident',\n",
    "    right_on='airport_ident',\n",
    "    how='left',\n",
    "    suffixes=('', '_from_runway')\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    runway_agg_df,\n",
    "    left_on='ident_to', \n",
    "    right_on='airport_ident',\n",
    "    how='left',\n",
    "    suffixes=('_from_runway', '_to_runway')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469f76c-4290-4c56-aaba-dbb86f7021ab",
   "metadata": {},
   "source": [
    "### Task 1b & 1c: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3daea4-d349-457b-a09f-4b0bc5e5df42",
   "metadata": {},
   "source": [
    "#### Scrape Airline Founding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa7aa51-573c-45ab-9f79-9e45b4ae1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_airlines_of_the_United_States\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2191e-b43f-4ff1-ad1a-1734f02f4c6e",
   "metadata": {},
   "source": [
    "#### Scrape Airport Hub Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8feb1ef1-3aef-4077-b777-7ad2b25b647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_hubs = \"https://en.wikipedia.org/wiki/List_of_the_busiest_airports_in_the_United_States\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d457e9c-4f4c-4f9a-83f4-f08e8cea3f66",
   "metadata": {},
   "source": [
    "### Task 2: Missing Value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf94f9-ce6d-4996-ad94-78e415a86593",
   "metadata": {},
   "source": [
    "#### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fec3b5a-d623-453c-8335-a93f0b542873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                       0\n",
      "Airline                                  0\n",
      "Flight                                   0\n",
      "AirportFrom                              0\n",
      "AirportTo                                0\n",
      "DayOfWeek                                0\n",
      "Time                                     0\n",
      "Length                                   0\n",
      "Delay                                    0\n",
      "id_from                                 31\n",
      "ident                                   31\n",
      "type                                    31\n",
      "name                                    31\n",
      "latitude_deg                            31\n",
      "longitude_deg                           31\n",
      "elevation_ft                            31\n",
      "continent                           509779\n",
      "iso_country                             31\n",
      "iso_region                              31\n",
      "municipality                            31\n",
      "scheduled_service                       31\n",
      "gps_code                                31\n",
      "iata_code                               31\n",
      "local_code                              31\n",
      "home_link                           136451\n",
      "wikipedia_link                        3110\n",
      "keywords                            259217\n",
      "id_to                                   31\n",
      "ident_to                                31\n",
      "type_to                                 31\n",
      "name_to                                 31\n",
      "latitude_deg_to                         31\n",
      "longitude_deg_to                        31\n",
      "elevation_ft_to                         31\n",
      "continent_to                        509749\n",
      "iso_country_to                          31\n",
      "iso_region_to                           31\n",
      "municipality_to                         31\n",
      "scheduled_service_to                    31\n",
      "gps_code_to                             31\n",
      "iata_code_to                            31\n",
      "local_code_to                           31\n",
      "home_link_to                        136356\n",
      "wikipedia_link_to                     3110\n",
      "keywords_to                         259179\n",
      "airport_ident_from_runway               31\n",
      "number_of_runways_from_runway           31\n",
      "avg_runway_length_ft_from_runway        31\n",
      "avg_runway_width_ft_from_runway        611\n",
      "airport_ident_to_runway                 31\n",
      "number_of_runways_to_runway             31\n",
      "avg_runway_length_ft_to_runway          31\n",
      "avg_runway_width_ft_to_runway          611\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbaf8d37-2c7f-4a7b-b438-2b93806bbeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Loading initial Excel files...\n",
      "Files loaded successfully.\n",
      "\n",
      " STEP 2: Aggregating runway data...\n",
      "Runway data aggregated.\n",
      "\n",
      " STEP 3: Merging flights with airport and runway data...\n",
      " All initial merges complete.\n",
      "\n",
      " STEP 4: Scraping, cleaning, and merging hub data...\n",
      " WARNING: Could not scrape or merge hub data. Details: \"None of [Index(['Airport', 'IATA'], dtype='object')] are in the [columns]\"\n",
      "   Creating placeholder columns for Hub_Size to allow script to continue.\n",
      "\n",
      "--- Verifying Columns Exist Before Imputation ---\n",
      "['id_x', 'Airline', 'Flight', 'AirportFrom', 'AirportTo', 'DayOfWeek', 'Time', 'Length', 'Delay', 'id_y', 'ident_from', 'type_from', 'name_from', 'latitude_deg_from', 'longitude_deg_from', 'elevation_ft_from', 'continent_from', 'iso_country_from', 'iso_region_from', 'municipality_from', 'scheduled_service_from', 'gps_code_from', 'iata_code_from', 'local_code_from', 'home_link_from', 'wikipedia_link_from', 'keywords_from', 'id', 'ident_to', 'type_to', 'name_to', 'latitude_deg_to', 'longitude_deg_to', 'elevation_ft_to', 'continent_to', 'iso_country_to', 'iso_region_to', 'municipality_to', 'scheduled_service_to', 'gps_code_to', 'iata_code_to', 'local_code_to', 'home_link_to', 'wikipedia_link_to', 'keywords_to', 'airport_ident_from_runway', 'number_of_runways_from_runway', 'avg_runway_length_ft_from_runway', 'avg_runway_width_ft_from_runway', 'airport_ident_to_runway', 'number_of_runways_to_runway', 'avg_runway_length_ft_to_runway', 'avg_runway_width_ft_to_runway', 'Hub_Size_from', 'Hub_Size_to']\n",
      "-------------------------------------------------\n",
      "\n",
      " STEP 5: Starting final data imputation...\n",
      "   Imputing missing elevation data...\n",
      " Elevation data imputed.\n",
      "   Imputing missing runway data...\n",
      "Runway data imputed.\n",
      "   Imputing missing hub status...\n",
      "Hub status imputed.\n",
      "\n",
      "\n",
      " ALL DATA PREPARATION COMPLETE. You can now proceed to visualization and modeling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['elevation_ft_from'].fillna(merged_df['elevation_ft_from'].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['elevation_ft_to'].fillna(merged_df['elevation_ft_to'].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_runway_length_ft_from_runway'].fillna(merged_df['avg_runway_length_ft_from_runway'].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['number_of_runways_from_runway'].fillna(0, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:73: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_runway_length_ft_to_runway'].fillna(merged_df['avg_runway_length_ft_to_runway'].median(), inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['number_of_runways_to_runway'].fillna(0, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Hub_Size_from'].fillna('Small/Non-Hub', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2240\\2430011939.py:79: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Hub_Size_to'].fillna('Small/Non-Hub', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "\n",
    "print(\"STEP 1: Loading initial Excel files...\")\n",
    "try:\n",
    "    flights_df = pd.read_excel(\"Airlines.xlsx\")\n",
    "    airports_df = pd.read_excel(\"airports.xlsx\")\n",
    "    runways_df = pd.read_excel(\"runways.xlsx\")\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FATAL ERROR: Could not find a file. Make sure they are in the correct folder. Details: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n STEP 2: Aggregating runway data...\")\n",
    "runway_agg_df = runways_df.groupby('airport_ident').agg(\n",
    "    number_of_runways=('id', 'count'),\n",
    "    avg_runway_length_ft=('length_ft', 'mean'),\n",
    "    avg_runway_width_ft=('width_ft', 'mean')\n",
    ").reset_index()\n",
    "print(\"Runway data aggregated.\")\n",
    "\n",
    "print(\"\\n STEP 3: Merging flights with airport and runway data...\")\n",
    "merged_df = pd.merge(flights_df, airports_df, left_on='AirportFrom', right_on='iata_code', how='left')\n",
    "merged_df = pd.merge(merged_df, airports_df, left_on='AirportTo', right_on='iata_code', how='left', suffixes=('_from', '_to'))\n",
    "merged_df = pd.merge(merged_df, runway_agg_df, left_on='ident_from', right_on='airport_ident', how='left', suffixes=('', '_from_runway'))\n",
    "merged_df = pd.merge(merged_df, runway_agg_df, left_on='ident_to', right_on='airport_ident', how='left', suffixes=('_from_runway', '_to_runway'))\n",
    "print(\" All initial merges complete.\")\n",
    "\n",
    "print(\"\\n STEP 4: Scraping, cleaning, and merging hub data...\")\n",
    "try:\n",
    "    url_hubs = \"https://en.wikipedia.org/wiki/List_of_the_busiest_airports_in_the_United_States\"\n",
    "    tables = pd.read_html(url_hubs)\n",
    "    hub_df = tables[0]\n",
    "    \n",
    "    if isinstance(hub_df.columns, pd.MultiIndex):\n",
    "        hub_df.columns = hub_df.columns.get_level_values(1)\n",
    "        \n",
    "    hub_df = hub_df[['Airport', 'IATA']]\n",
    "    hub_df['Hub_Size'] = 'Small/Non-Hub'\n",
    "    hub_df.loc[hub_df.index < 30, 'Hub_Size'] = 'Large Hub'\n",
    "    hub_df.loc[(hub_df.index >= 30) & (hub_df.index < 70), 'Hub_Size'] = 'Medium Hub'\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, hub_df[['IATA', 'Hub_Size']], left_on='AirportFrom', right_on='IATA', how='left')\n",
    "    merged_df.rename(columns={'Hub_Size': 'Hub_Size_from'}, inplace=True)\n",
    "    # Merge for destination\n",
    "    merged_df = pd.merge(merged_df, hub_df[['IATA', 'Hub_Size']], left_on='AirportTo', right_on='IATA', how='left')\n",
    "    merged_df.rename(columns={'Hub_Size': 'Hub_Size_to'}, inplace=True)\n",
    "    \n",
    "    merged_df.drop(columns=['IATA_x', 'IATA_y'], inplace=True, errors='ignore')\n",
    "    print(\" Hub data scraped and merged successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\" WARNING: Could not scrape or merge hub data. Details: {e}\")\n",
    "    print(\"   Creating placeholder columns for Hub_Size to allow script to continue.\")\n",
    "    merged_df['Hub_Size_from'] = 'Unknown'\n",
    "    merged_df['Hub_Size_to'] = 'Unknown'\n",
    "\n",
    "print(\"\\n--- Verifying Columns Exist Before Imputation ---\")\n",
    "print(merged_df.columns.tolist())\n",
    "print(\"-------------------------------------------------\")\n",
    "\n",
    "print(\"\\n STEP 5: Starting final data imputation...\")\n",
    "\n",
    "print(\"   Imputing missing elevation data...\")\n",
    "merged_df['elevation_ft_from'].fillna(merged_df['elevation_ft_from'].median(), inplace=True)\n",
    "merged_df['elevation_ft_to'].fillna(merged_df['elevation_ft_to'].median(), inplace=True)\n",
    "print(\" Elevation data imputed.\")\n",
    "\n",
    "print(\"   Imputing missing runway data...\")\n",
    "merged_df['avg_runway_length_ft_from_runway'].fillna(merged_df['avg_runway_length_ft_from_runway'].median(), inplace=True)\n",
    "merged_df['number_of_runways_from_runway'].fillna(0, inplace=True)\n",
    "merged_df['avg_runway_length_ft_to_runway'].fillna(merged_df['avg_runway_length_ft_to_runway'].median(), inplace=True)\n",
    "merged_df['number_of_runways_to_runway'].fillna(0, inplace=True)\n",
    "print(\"Runway data imputed.\")\n",
    "\n",
    "print(\"   Imputing missing hub status...\")\n",
    "merged_df['Hub_Size_from'].fillna('Small/Non-Hub', inplace=True)\n",
    "merged_df['Hub_Size_to'].fillna('Small/Non-Hub', inplace=True)\n",
    "print(\"Hub status imputed.\")\n",
    "\n",
    "print(\"\\n\\n ALL DATA PREPARATION COMPLETE. You can now proceed to visualization and modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a984a2-562e-48ab-b86d-508077016488",
   "metadata": {},
   "source": [
    "### Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59681e-220f-4f8a-8fe3-49fb57ee548e",
   "metadata": {},
   "source": [
    "#### Import Libraries and Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f02afca4-6c9d-4b36-b60b-038d7653911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All machine learning libraries imported successfully.\n",
      "\n",
      "Features (X) shape: (518556, 9)\n",
      "Target (y) shape: (518556,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\" All machine learning libraries imported successfully.\")\n",
    "\n",
    "features = [\n",
    "    'Airline', 'AirportFrom', 'AirportTo', 'DayOfWeek', 'Time', 'Length',\n",
    "    'elevation_ft_from', 'number_of_runways_from_runway', 'Hub_Size_from'\n",
    "]\n",
    "\n",
    "target = 'Delay'\n",
    "\n",
    "X = merged_df[features]\n",
    "y = merged_df[target]\n",
    "\n",
    "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f4cd0a-cb72-4cfb-b2b7-2b08b3af2a3a",
   "metadata": {},
   "source": [
    "#### Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "262591ba-f807-4470-8f34-00ab0143f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets:\n",
      "X_train shape: (414844, 9)\n",
      "X_test shape: (103712, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,      \n",
    "    random_state=42,   \n",
    "    stratify=y          \n",
    ")\n",
    "\n",
    "print(\"Data split into training and testing sets:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6646d5-3328-4559-b83d-9f57a93b8a3e",
   "metadata": {},
   "source": [
    "####  Create the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4909bd69-bfef-4746-82a2-5ecd47d2112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "categorical_features = ['Airline', 'AirportFrom', 'AirportTo', 'DayOfWeek', 'Hub_Size_from']\n",
    "numerical_features = ['Time', 'Length', 'elevation_ft_from', 'number_of_runways_from_runway']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89eb582-d436-44b2-a652-8ef0dfb072f6",
   "metadata": {},
   "source": [
    "####  Build, Train, and Evaluate Model 1 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0411ff2-e106-413b-8e94-55f27350ac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Logistic Regression model...\n",
      "Training complete.\n",
      "\n",
      "--- Logistic Regression Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70     56914\n",
      "           1       0.62      0.51      0.56     46798\n",
      "\n",
      "    accuracy                           0.64    103712\n",
      "   macro avg       0.64      0.63      0.63    103712\n",
      "weighted avg       0.64      0.64      0.63    103712\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SGDClassifier(loss='log_loss', random_state=42, max_iter=1000, tol=1e-3))\n",
    "])\n",
    "\n",
    "print(\"Training the Logistic Regression model...\")\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Logistic Regression Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5856b-1b22-4ab0-9626-df6e5109d1d4",
   "metadata": {},
   "source": [
    "#### Build, Train, and Evaluate Model 2 (Decision Tree with Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43f6da74-f186-4127-a461-0587efad2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for the best Decision Tree parameters (this may take a moment)...\n",
      "Search complete.\n",
      "Best parameters found: {'classifier__max_depth': 15, 'classifier__min_samples_leaf': 50}\n",
      "\n",
      "--- Tuned Decision Tree Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.72     56914\n",
      "           1       0.66      0.47      0.55     46798\n",
      "\n",
      "    accuracy                           0.65    103712\n",
      "   macro avg       0.65      0.64      0.63    103712\n",
      "weighted avg       0.65      0.65      0.64    103712\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dt_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__min_samples_leaf': [10, 20, 50]\n",
    "}\n",
    "\n",
    "print(\"\\nSearching for the best Decision Tree parameters (this may take a moment)...\")\n",
    "grid_search_dt = GridSearchCV(dt_pipeline, param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "print(\"Search complete.\")\n",
    "print(f\"Best parameters found: {grid_search_dt.best_params_}\")\n",
    "\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "y_pred_dt = best_dt_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Tuned Decision Tree Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f559f9-fad4-4735-936e-9f900d670c63",
   "metadata": {},
   "source": [
    "#### Build, Train, and Evaluate Model 3 (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16ccde5d-6823-427d-bddc-906806ea0208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the Gradient Boosting model (this can be slow)...\n",
      "Training complete.\n",
      "\n",
      "--- Gradient Boosting Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.72     56914\n",
      "           1       0.68      0.42      0.52     46798\n",
      "\n",
      "    accuracy                           0.65    103712\n",
      "   macro avg       0.66      0.63      0.62    103712\n",
      "weighted avg       0.66      0.65      0.63    103712\n",
      "\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining the Gradient Boosting model (this can be slow)...\")\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Gradient Boosting Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02881d-d0bc-4806-9877-fc826a73b3ec",
   "metadata": {},
   "source": [
    "#### Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e496a3-e3c8-4711-9a84-2d15db579534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Performance Summary ---\n",
      "                 Model  Test Accuracy\n",
      "1  Tuned Decision Tree       0.651101\n",
      "2    Gradient Boosting       0.648151\n",
      "0  Logistic Regression       0.640302\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "acc_gb = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Tuned Decision Tree', 'Gradient Boosting'],\n",
    "    'Test Accuracy': [acc_lr, acc_dt, acc_gb]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Final Model Performance Summary ---\")\n",
    "print(summary.sort_values(by='Test Accuracy', ascending=False))\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adefaf80-c98c-4ef6-968d-494a9a26b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preparing to extract the final dataset...\n",
      "   The final DataFrame has 518556 rows and 55 columns.\n",
      "\n",
      " Success! The final dataset has been extracted and saved as 'final_airline_delay_dataset.csv'\n",
      "   You can now find this file in the same directory as your Jupyter Notebook.\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"final_airline_delay_dataset.csv\"\n",
    "\n",
    "print(f\" Preparing to extract the final dataset...\")\n",
    "print(f\"   The final DataFrame has {merged_df.shape[0]} rows and {merged_df.shape[1]} columns.\")\n",
    "\n",
    "try:\n",
    "    merged_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"\\n Success! The final dataset has been extracted and saved as '{output_filename}'\")\n",
    "    print(\"   You can now find this file in the same directory as your Jupyter Notebook.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n An error occurred while trying to save the file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fcfedb2-2672-4a45-959a-2d76f8d08253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Re-exporting the final dataset to fix the header issue...\n",
      "\n",
      " Success! The file 'final_airline_delay_dataset(1).csv' has been re-saved correctly without the index column.\n"
     ]
    }
   ],
   "source": [
    "final_dataframe_to_export = merged_df\n",
    "\n",
    "output_filename = \"final_airline_delay_dataset(1).csv\"\n",
    "\n",
    "print(f\" Re-exporting the final dataset to fix the header issue...\")\n",
    "\n",
    "final_dataframe_to_export.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n Success! The file '{output_filename}' has been re-saved correctly without the index column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6aa0889-2de9-4893-b99b-b3c0a9a2c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: final_airline_delay_dataset(1).csv\n",
      "\n",
      "--- Columns found in the CSV file ---\n",
      "['id_x', 'Airline', 'Flight', 'AirportFrom', 'AirportTo', 'DayOfWeek', 'Time', 'Length', 'Delay', 'id_y', 'ident_from', 'type_from', 'name_from', 'latitude_deg_from', 'longitude_deg_from', 'elevation_ft_from', 'continent_from', 'iso_country_from', 'iso_region_from', 'municipality_from', 'scheduled_service_from', 'gps_code_from', 'iata_code_from', 'local_code_from', 'home_link_from', 'wikipedia_link_from', 'keywords_from', 'id', 'ident_to', 'type_to', 'name_to', 'latitude_deg_to', 'longitude_deg_to', 'elevation_ft_to', 'continent_to', 'iso_country_to', 'iso_region_to', 'municipality_to', 'scheduled_service_to', 'gps_code_to', 'iata_code_to', 'local_code_to', 'home_link_to', 'wikipedia_link_to', 'keywords_to', 'airport_ident_from_runway', 'number_of_runways_from_runway', 'avg_runway_length_ft_from_runway', 'avg_runway_width_ft_from_runway', 'airport_ident_to_runway', 'number_of_runways_to_runway', 'avg_runway_length_ft_to_runway', 'avg_runway_width_ft_to_runway', 'Hub_Size_from', 'Hub_Size_to']\n",
      "\n",
      "Total number of columns in CSV: 55\n",
      "\n",
      " Good news: No columns are named 'Unnamed'. The headers seem clean.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"final_airline_delay_dataset(1).csv\"\n",
    "print(f\"Inspecting file: {filename}\\n\")\n",
    "\n",
    "try:\n",
    "    df_to_inspect = pd.read_csv(filename)\n",
    "\n",
    "    csv_columns = df_to_inspect.columns.tolist()\n",
    "    \n",
    "    print(\"--- Columns found in the CSV file ---\")\n",
    "    print(csv_columns)\n",
    "    print(f\"\\nTotal number of columns in CSV: {len(csv_columns)}\")\n",
    "    \n",
    "    unnamed_cols = [col for col in csv_columns if 'Unnamed' in col]\n",
    "    \n",
    "    if unnamed_cols:\n",
    "        print(f\"\\n CRITICAL ISSUE FOUND: The CSV contains unnamed columns: {unnamed_cols}\")\n",
    "        print(\"   This is the most likely cause of the error. These must be removed.\")\n",
    "    else:\n",
    "        print(\"\\n Good news: No columns are named 'Unnamed'. The headers seem clean.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" An error occurred while trying to read the CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557343b-1edc-4a22-8c53-d27c3ed5223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy.dialects.mysql import LONGTEXT\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "try:\n",
    "    db_user = 'root'\n",
    "    db_password = 'Pu*TM3vq7W9r' \n",
    "    db_host = 'localhost'\n",
    "    db_name = 'airlines_db'\n",
    "\n",
    "    connection_string = f\"mysql+mysqlconnector://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    print(\" Database connection engine created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error creating database connection: {e}\")\n",
    "    raise\n",
    "\n",
    "filename = \"final_airline_delay_dataset(1).csv\"\n",
    "print(f\"\\nLoading data from {filename}...\")\n",
    "df_to_load = pd.read_csv(filename)\n",
    "\n",
    "print(\"\\nCreating explicit dtype mapping for SQL table...\")\n",
    "all_columns = df_to_load.columns\n",
    "dtype_mapping = {}\n",
    "for col in all_columns:\n",
    "    if df_to_load[col].dtype == 'object':\n",
    "        dtype_mapping[col] = types.VARCHAR(length=255)\n",
    "    elif 'int' in str(df_to_load[col].dtype):\n",
    "        dtype_mapping[col] = types.BIGINT\n",
    "    elif 'float' in str(df_to_load[col].dtype):\n",
    "        dtype_mapping[col] = types.DECIMAL(precision=10, scale=2)\n",
    "\n",
    "long_text_columns = [\n",
    "    'keywords_from', 'keywords_to', 'name_from', 'name_to', \n",
    "    'home_link_from', 'home_link_to', 'wikipedia_link_from', 'wikipedia_link_to'\n",
    "]\n",
    "for col in long_text_columns:\n",
    "    if col in dtype_mapping:\n",
    "        dtype_mapping[col] = LONGTEXT\n",
    "print(\" Dtype mapping created successfully.\")\n",
    "\n",
    "\n",
    "chunk_size = 10000 \n",
    "table_name = 'final_data'\n",
    "start_row = 0\n",
    "\n",
    "try:\n",
    "    print(f\"\\nCreating a new, empty table named '{table_name}'...\")\n",
    "    df_to_load.head(0).to_sql(\n",
    "        name=table_name, \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        index=False,\n",
    "        dtype=dtype_mapping\n",
    "    )\n",
    "    print(\" Empty table created successfully.\")\n",
    "\n",
    "    print(\"\\nStarting to load data in chunks...\")\n",
    "    for i in range(0, len(df_to_load), chunk_size):\n",
    "        start_row = i\n",
    "        end_row = i + chunk_size\n",
    "        chunk = df_to_load.iloc[start_row:end_row]\n",
    "        \n",
    "        print(f\"  - Loading rows {start_row} to {end_row-1}...\")\n",
    "        \n",
    "        chunk.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    print(\"\\n SUCCESS! All chunks loaded successfully.\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"\\n A SQLAlchemy error occurred while processing the chunk starting at row {start_row}.\")\n",
    "    print(f\"      The problematic data is likely between row {start_row} and {start_row + chunk_size - 1} in your CSV file.\")\n",
    "    print(f\"      Original Error: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n An unexpected error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    engine.dispose()\n",
    "    print(\"\\nDatabase connection engine disposed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b784b-f7aa-466a-9d1e-838867483579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
